{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance of hyperparameters\n",
    "\n",
    "Deep reinforcement learning more sensitive to hyperparameters than supervised learning\n",
    "More variability in convergence across seeds as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 410       |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.7      |\n",
      "|    critic_loss     | 1.46      |\n",
      "|    ent_coef        | 0.811     |\n",
      "|    ent_coef_loss   | -0.346    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 388       |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.3      |\n",
      "|    critic_loss     | 0.961     |\n",
      "|    ent_coef        | 0.645     |\n",
      "|    ent_coef_loss   | -0.661    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 381       |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.7      |\n",
      "|    critic_loss     | 0.746     |\n",
      "|    ent_coef        | 0.52      |\n",
      "|    ent_coef_loss   | -0.807    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 378       |\n",
      "|    time_elapsed    | 8         |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 79.2      |\n",
      "|    critic_loss     | 2.42      |\n",
      "|    ent_coef        | 0.429     |\n",
      "|    ent_coef_loss   | -0.639    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 378       |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 104       |\n",
      "|    critic_loss     | 4.12      |\n",
      "|    ent_coef        | 0.364     |\n",
      "|    ent_coef_loss   | -0.667    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.35e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 377       |\n",
      "|    time_elapsed    | 12        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 113       |\n",
      "|    critic_loss     | 3.2       |\n",
      "|    ent_coef        | 0.312     |\n",
      "|    ent_coef_loss   | -0.53     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4699      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 377       |\n",
      "|    time_elapsed    | 14        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 132       |\n",
      "|    critic_loss     | 2.33      |\n",
      "|    ent_coef        | 0.252     |\n",
      "|    ent_coef_loss   | -0.771    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 377       |\n",
      "|    time_elapsed    | 16        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 138       |\n",
      "|    critic_loss     | 2.58      |\n",
      "|    ent_coef        | 0.204     |\n",
      "|    ent_coef_loss   | -0.467    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 376       |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 162       |\n",
      "|    critic_loss     | 3.08      |\n",
      "|    ent_coef        | 0.183     |\n",
      "|    ent_coef_loss   | 0.151     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 376       |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 160       |\n",
      "|    critic_loss     | 3.32      |\n",
      "|    ent_coef        | 0.19      |\n",
      "|    ent_coef_loss   | 0.0809    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    verbose=1,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    policy_kwargs=dict(net_arch=[64,64]),\n",
    ").learn(8_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-329.61 +/- 106.49\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 81        |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 23.4      |\n",
      "|    critic_loss     | 0.237     |\n",
      "|    ent_coef        | 0.813     |\n",
      "|    ent_coef_loss   | -0.341    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 75        |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 47.6      |\n",
      "|    critic_loss     | 0.183     |\n",
      "|    ent_coef        | 0.647     |\n",
      "|    ent_coef_loss   | -0.599    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 73        |\n",
      "|    time_elapsed    | 32        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 64.8      |\n",
      "|    critic_loss     | 0.256     |\n",
      "|    ent_coef        | 0.531     |\n",
      "|    ent_coef_loss   | -0.603    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.18e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 73        |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 74.7      |\n",
      "|    critic_loss     | 0.376     |\n",
      "|    ent_coef        | 0.452     |\n",
      "|    ent_coef_loss   | -0.498    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -975     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 73       |\n",
      "|    critic_loss     | 0.514    |\n",
      "|    ent_coef        | 0.389    |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -829     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 77       |\n",
      "|    critic_loss     | 0.635    |\n",
      "|    ent_coef        | 0.327    |\n",
      "|    ent_coef_loss   | -0.606   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -737     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.9     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.269    |\n",
      "|    ent_coef_loss   | -0.777   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -660     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.7     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.531   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -601     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 77       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66.1     |\n",
      "|    critic_loss     | 0.778    |\n",
      "|    ent_coef        | 0.182    |\n",
      "|    ent_coef_loss   | -0.585   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -550     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 77       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66.3     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -0.522   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    \"Pendulum-v1\",\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(net_arch=[256,256]),\n",
    "    seed=0,\n",
    ").learn(8_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.pyenv/versions/3.11.3/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -358.32 +/- 363.50\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=100)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "Class from BaseCallback\n",
    "\n",
    "Gives access to events: _on_training_start, _on_step  \n",
    "Gives access to variables: self.model\n",
    "\n",
    "Can change variables without halting training or changing code\n",
    "\n",
    "Variables eaccessible in callback:  \n",
    "* self.model: BaseRLModel\n",
    "* self.training_env = None: Union[gym.Env, VecEnv, None]\n",
    "* self.n_calls = 0: int\n",
    "* self.num_timesteps = 0: int\n",
    "* self.locals = none: Dict[str, Any]\n",
    "* self.globals = None: Dict[str, Any]\n",
    "* self.logger = None: logger.Logger\n",
    "* self.parent = None: Optional[BaseCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton Callback example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback from BaseCallback\n",
    "    :param verbose: (int) Verbosity level 0: no output, 1: info, 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Triggered before first rollout\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Rollout: collection of environment interactions using current policy\n",
    "        This method triggered before collecting new samples\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called by model after each env.step()\n",
    "        For child callback of an EventCallback, called when event triggered\n",
    "        :return: (bool) False if training is aborted early\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Triggered after rollout before updating policy\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Triggered before exiting learn()\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Callback that can only be called twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Can only be called twice\n",
    "    :param verbose: verbosity level 0: none, 1: info, 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SimpleCallback, self).__init__(verbose)\n",
    "        self._called = False\n",
    "\n",
    "    def _on_step(self):\n",
    "        if not self._called:\n",
    "            print(\"callback: first call\")\n",
    "            self._called = True\n",
    "            return True # training continues\n",
    "        print(\"callback: second call\")\n",
    "        return False # training stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "callback: first call\n",
      "callback: second call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f83791f0d10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n",
    "model.learn(8000, callback=SimpleCallback())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autosave Best Model\n",
    "\n",
    "Track training statistics using Monitor wrapper\n",
    "\n",
    "Save best model based on training reward\n",
    "\n",
    "Note: proper form is use callback to test agent in evaluation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModelCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving model with best training reward.\n",
    "    Check done every ''check_freq'' steps.\n",
    "    Should use ''EvalCallback'' in practice.\n",
    "    \n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to model dir.\n",
    "        Must contain file created by ''Monitor'' wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "\n",
    "                # Mean training reward over last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2f}, \\\n",
    "                          Last mean reward: {mean_reward:.2f}\")\n",
    "                    \n",
    "                # Save on new best model\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model at {x[-1]} timesteps to {self.save_path}.zip\")\n",
    "\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 40\n",
      "Best mean reward: -inf,                           Last mean reward: 36.00\n",
      "Saving new best model at 36 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 60\n",
      "Best mean reward: 36.00,                           Last mean reward: 36.00\n",
      "Num timesteps: 80\n",
      "Best mean reward: 36.00,                           Last mean reward: 37.50\n",
      "Saving new best model at 75 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 100\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.00\n",
      "Num timesteps: 120\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.25\n",
      "Num timesteps: 140\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.25\n",
      "Num timesteps: 160\n",
      "Best mean reward: 37.50,                           Last mean reward: 30.00\n",
      "Num timesteps: 180\n",
      "Best mean reward: 37.50,                           Last mean reward: 27.33\n",
      "Num timesteps: 200\n",
      "Best mean reward: 37.50,                           Last mean reward: 26.00\n",
      "Num timesteps: 220\n",
      "Best mean reward: 37.50,                           Last mean reward: 23.56\n",
      "Num timesteps: 240\n",
      "Best mean reward: 37.50,                           Last mean reward: 23.70\n",
      "Num timesteps: 260\n",
      "Best mean reward: 37.50,                           Last mean reward: 23.70\n",
      "Num timesteps: 280\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.45\n",
      "Num timesteps: 300\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.45\n",
      "Num timesteps: 320\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.83\n",
      "Num timesteps: 340\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.83\n",
      "Num timesteps: 360\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.43\n",
      "Num timesteps: 380\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.43\n",
      "Num timesteps: 400\n",
      "Best mean reward: 37.50,                           Last mean reward: 25.43\n",
      "Num timesteps: 420\n",
      "Best mean reward: 37.50,                           Last mean reward: 27.13\n",
      "Num timesteps: 440\n",
      "Best mean reward: 37.50,                           Last mean reward: 27.13\n",
      "Num timesteps: 460\n",
      "Best mean reward: 37.50,                           Last mean reward: 27.13\n",
      "Num timesteps: 480\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.12\n",
      "Num timesteps: 500\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.12\n",
      "Num timesteps: 520\n",
      "Best mean reward: 37.50,                           Last mean reward: 29.12\n",
      "Num timesteps: 540\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.41\n",
      "Num timesteps: 560\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.41\n",
      "Num timesteps: 580\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.22\n",
      "Num timesteps: 600\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.22\n",
      "Num timesteps: 620\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.22\n",
      "Num timesteps: 640\n",
      "Best mean reward: 37.50,                           Last mean reward: 31.22\n",
      "Num timesteps: 660\n",
      "Best mean reward: 37.50,                           Last mean reward: 33.79\n",
      "Num timesteps: 680\n",
      "Best mean reward: 37.50,                           Last mean reward: 33.25\n",
      "Num timesteps: 700\n",
      "Best mean reward: 37.50,                           Last mean reward: 33.25\n",
      "Num timesteps: 720\n",
      "Best mean reward: 37.50,                           Last mean reward: 34.24\n",
      "Num timesteps: 740\n",
      "Best mean reward: 37.50,                           Last mean reward: 34.24\n",
      "Num timesteps: 760\n",
      "Best mean reward: 37.50,                           Last mean reward: 34.24\n",
      "Num timesteps: 780\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.00\n",
      "Num timesteps: 800\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.00\n",
      "Num timesteps: 820\n",
      "Best mean reward: 37.50,                           Last mean reward: 34.91\n",
      "Num timesteps: 840\n",
      "Best mean reward: 37.50,                           Last mean reward: 34.91\n",
      "Num timesteps: 860\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.46\n",
      "Num timesteps: 880\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.46\n",
      "Num timesteps: 900\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.32\n",
      "Num timesteps: 920\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.31\n",
      "Num timesteps: 940\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.31\n",
      "Num timesteps: 960\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.31\n",
      "Num timesteps: 980\n",
      "Best mean reward: 37.50,                           Last mean reward: 35.31\n",
      "Num timesteps: 1000\n",
      "Best mean reward: 37.50,                           Last mean reward: 36.63\n",
      "Num timesteps: 1020\n",
      "Best mean reward: 37.50,                           Last mean reward: 36.63\n",
      "Num timesteps: 1040\n",
      "Best mean reward: 37.50,                           Last mean reward: 36.63\n",
      "Num timesteps: 1060\n",
      "Best mean reward: 37.50,                           Last mean reward: 36.63\n",
      "Num timesteps: 1080\n",
      "Best mean reward: 37.50,                           Last mean reward: 38.04\n",
      "Saving new best model at 1065 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1100\n",
      "Best mean reward: 38.04,                           Last mean reward: 38.04\n",
      "Num timesteps: 1120\n",
      "Best mean reward: 38.04,                           Last mean reward: 38.28\n",
      "Saving new best model at 1110 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1140\n",
      "Best mean reward: 38.28,                           Last mean reward: 37.93\n",
      "Num timesteps: 1160\n",
      "Best mean reward: 38.28,                           Last mean reward: 37.93\n",
      "Num timesteps: 1180\n",
      "Best mean reward: 38.28,                           Last mean reward: 37.93\n",
      "Num timesteps: 1200\n",
      "Best mean reward: 38.28,                           Last mean reward: 38.45\n",
      "Saving new best model at 1192 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1220\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.45\n",
      "Num timesteps: 1240\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.31\n",
      "Num timesteps: 1260\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.03\n",
      "Num timesteps: 1280\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.03\n",
      "Num timesteps: 1300\n",
      "Best mean reward: 38.45,                           Last mean reward: 37.82\n",
      "Num timesteps: 1320\n",
      "Best mean reward: 38.45,                           Last mean reward: 37.82\n",
      "Num timesteps: 1340\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.23\n",
      "Num timesteps: 1360\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.23\n",
      "Num timesteps: 1380\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.23\n",
      "Num timesteps: 1400\n",
      "Best mean reward: 38.45,                           Last mean reward: 38.61\n",
      "Saving new best model at 1390 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1420\n",
      "Best mean reward: 38.61,                           Last mean reward: 38.61\n",
      "Num timesteps: 1440\n",
      "Best mean reward: 38.61,                           Last mean reward: 38.61\n",
      "Num timesteps: 1460\n",
      "Best mean reward: 38.61,                           Last mean reward: 39.24\n",
      "Saving new best model at 1452 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1480\n",
      "Best mean reward: 39.24,                           Last mean reward: 39.24\n",
      "Num timesteps: 1500\n",
      "Best mean reward: 39.24,                           Last mean reward: 39.24\n",
      "Num timesteps: 1520\n",
      "Best mean reward: 39.24,                           Last mean reward: 39.50\n",
      "Saving new best model at 1501 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1540\n",
      "Best mean reward: 39.50,                           Last mean reward: 39.50\n",
      "Num timesteps: 1560\n",
      "Best mean reward: 39.50,                           Last mean reward: 39.50\n",
      "Num timesteps: 1580\n",
      "Best mean reward: 39.50,                           Last mean reward: 40.03\n",
      "Saving new best model at 1561 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1600\n",
      "Best mean reward: 40.03,                           Last mean reward: 40.03\n",
      "Num timesteps: 1620\n",
      "Best mean reward: 40.03,                           Last mean reward: 40.30\n",
      "Saving new best model at 1612 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1640\n",
      "Best mean reward: 40.30,                           Last mean reward: 40.30\n",
      "Num timesteps: 1660\n",
      "Best mean reward: 40.30,                           Last mean reward: 40.30\n",
      "Num timesteps: 1680\n",
      "Best mean reward: 40.30,                           Last mean reward: 40.30\n",
      "Num timesteps: 1700\n",
      "Best mean reward: 40.30,                           Last mean reward: 40.30\n",
      "Num timesteps: 1720\n",
      "Best mean reward: 40.30,                           Last mean reward: 40.30\n",
      "Num timesteps: 1740\n",
      "Best mean reward: 40.30,                           Last mean reward: 42.10\n",
      "Saving new best model at 1726 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1760\n",
      "Best mean reward: 42.10,                           Last mean reward: 42.10\n",
      "Num timesteps: 1780\n",
      "Best mean reward: 42.10,                           Last mean reward: 42.10\n",
      "Num timesteps: 1800\n",
      "Best mean reward: 42.10,                           Last mean reward: 42.10\n",
      "Num timesteps: 1820\n",
      "Best mean reward: 42.10,                           Last mean reward: 42.10\n",
      "Num timesteps: 1840\n",
      "Best mean reward: 42.10,                           Last mean reward: 42.10\n",
      "Num timesteps: 1860\n",
      "Best mean reward: 42.10,                           Last mean reward: 44.10\n",
      "Saving new best model at 1852 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1880\n",
      "Best mean reward: 44.10,                           Last mean reward: 44.10\n",
      "Num timesteps: 1900\n",
      "Best mean reward: 44.10,                           Last mean reward: 43.79\n",
      "Num timesteps: 1920\n",
      "Best mean reward: 44.10,                           Last mean reward: 43.79\n",
      "Num timesteps: 1940\n",
      "Best mean reward: 44.10,                           Last mean reward: 43.79\n",
      "Num timesteps: 1960\n",
      "Best mean reward: 44.10,                           Last mean reward: 44.41\n",
      "Saving new best model at 1954 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 1980\n",
      "Best mean reward: 44.41,                           Last mean reward: 44.41\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 44.41,                           Last mean reward: 44.41\n",
      "Num timesteps: 2020\n",
      "Best mean reward: 44.41,                           Last mean reward: 44.41\n",
      "Num timesteps: 2040\n",
      "Best mean reward: 44.41,                           Last mean reward: 45.13\n",
      "Saving new best model at 2031 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2060\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.13\n",
      "Num timesteps: 2080\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2100\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2120\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2140\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2160\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2180\n",
      "Best mean reward: 45.13,                           Last mean reward: 45.07\n",
      "Num timesteps: 2200\n",
      "Best mean reward: 45.13,                           Last mean reward: 46.47\n",
      "Saving new best model at 2184 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2220\n",
      "Best mean reward: 46.47,                           Last mean reward: 46.47\n",
      "Num timesteps: 2240\n",
      "Best mean reward: 46.47,                           Last mean reward: 46.47\n",
      "Num timesteps: 2260\n",
      "Best mean reward: 46.47,                           Last mean reward: 46.79\n",
      "Saving new best model at 2246 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2280\n",
      "Best mean reward: 46.79,                           Last mean reward: 46.79\n",
      "Num timesteps: 2300\n",
      "Best mean reward: 46.79,                           Last mean reward: 46.79\n",
      "Num timesteps: 2320\n",
      "Best mean reward: 46.79,                           Last mean reward: 46.79\n",
      "Num timesteps: 2340\n",
      "Best mean reward: 46.79,                           Last mean reward: 46.79\n",
      "Num timesteps: 2360\n",
      "Best mean reward: 46.79,                           Last mean reward: 46.79\n",
      "Num timesteps: 2380\n",
      "Best mean reward: 46.79,                           Last mean reward: 48.29\n",
      "Saving new best model at 2366 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2400\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.29\n",
      "Num timesteps: 2420\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.24\n",
      "Num timesteps: 2440\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.24\n",
      "Num timesteps: 2460\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.24\n",
      "Num timesteps: 2480\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.24\n",
      "Num timesteps: 2500\n",
      "Best mean reward: 48.29,                           Last mean reward: 48.69\n",
      "Saving new best model at 2483 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2520\n",
      "Best mean reward: 48.69,                           Last mean reward: 48.69\n",
      "Num timesteps: 2540\n",
      "Best mean reward: 48.69,                           Last mean reward: 48.77\n",
      "Saving new best model at 2536 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2560\n",
      "Best mean reward: 48.77,                           Last mean reward: 48.77\n",
      "Num timesteps: 2580\n",
      "Best mean reward: 48.77,                           Last mean reward: 48.77\n",
      "Num timesteps: 2600\n",
      "Best mean reward: 48.77,                           Last mean reward: 48.77\n",
      "Num timesteps: 2620\n",
      "Best mean reward: 48.77,                           Last mean reward: 48.77\n",
      "Num timesteps: 2640\n",
      "Best mean reward: 48.77,                           Last mean reward: 48.77\n",
      "Num timesteps: 2660\n",
      "Best mean reward: 48.77,                           Last mean reward: 49.96\n",
      "Saving new best model at 2648 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2680\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.96\n",
      "Num timesteps: 2700\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.83\n",
      "Num timesteps: 2720\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.83\n",
      "Num timesteps: 2740\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.83\n",
      "Num timesteps: 2760\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.91\n",
      "Num timesteps: 2780\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.91\n",
      "Num timesteps: 2800\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.91\n",
      "Num timesteps: 2820\n",
      "Best mean reward: 49.96,                           Last mean reward: 49.91\n",
      "Num timesteps: 2840\n",
      "Best mean reward: 49.96,                           Last mean reward: 50.45\n",
      "Saving new best model at 2825 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2860\n",
      "Best mean reward: 50.45,                           Last mean reward: 50.45\n",
      "Num timesteps: 2880\n",
      "Best mean reward: 50.45,                           Last mean reward: 50.45\n",
      "Num timesteps: 2900\n",
      "Best mean reward: 50.45,                           Last mean reward: 50.75\n",
      "Saving new best model at 2893 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2920\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.75\n",
      "Num timesteps: 2940\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.57\n",
      "Num timesteps: 2960\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.57\n",
      "Num timesteps: 2980\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.57\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.59\n",
      "Num timesteps: 3020\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.59\n",
      "Num timesteps: 3040\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.48\n",
      "Num timesteps: 3060\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.48\n",
      "Num timesteps: 3080\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.44\n",
      "Num timesteps: 3100\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.44\n",
      "Num timesteps: 3120\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.44\n",
      "Num timesteps: 3140\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.44\n",
      "Num timesteps: 3160\n",
      "Best mean reward: 50.75,                           Last mean reward: 50.95\n",
      "Saving new best model at 3159 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3180\n",
      "Best mean reward: 50.95,                           Last mean reward: 50.95\n",
      "Num timesteps: 3200\n",
      "Best mean reward: 50.95,                           Last mean reward: 50.95\n",
      "Num timesteps: 3220\n",
      "Best mean reward: 50.95,                           Last mean reward: 50.95\n",
      "Num timesteps: 3240\n",
      "Best mean reward: 50.95,                           Last mean reward: 51.21\n",
      "Saving new best model at 3226 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3260\n",
      "Best mean reward: 51.21,                           Last mean reward: 51.21\n",
      "Num timesteps: 3280\n",
      "Best mean reward: 51.21,                           Last mean reward: 51.21\n",
      "Num timesteps: 3300\n",
      "Best mean reward: 51.21,                           Last mean reward: 51.21\n",
      "Num timesteps: 3320\n",
      "Best mean reward: 51.21,                           Last mean reward: 51.73\n",
      "Saving new best model at 3311 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3340\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.34\n",
      "Num timesteps: 3360\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.34\n",
      "Num timesteps: 3380\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.34\n",
      "Num timesteps: 3400\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.34\n",
      "Num timesteps: 3420\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.34\n",
      "Num timesteps: 3440\n",
      "Best mean reward: 51.73,                           Last mean reward: 51.85\n",
      "Saving new best model at 3422 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3460\n",
      "Best mean reward: 51.85,                           Last mean reward: 51.85\n",
      "Num timesteps: 3480\n",
      "Best mean reward: 51.85,                           Last mean reward: 51.85\n",
      "Num timesteps: 3500\n",
      "Best mean reward: 51.85,                           Last mean reward: 51.85\n",
      "Num timesteps: 3520\n",
      "Best mean reward: 51.85,                           Last mean reward: 51.85\n",
      "Num timesteps: 3540\n",
      "Best mean reward: 51.85,                           Last mean reward: 52.82\n",
      "Saving new best model at 3539 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3560\n",
      "Best mean reward: 52.82,                           Last mean reward: 52.82\n",
      "Num timesteps: 3580\n",
      "Best mean reward: 52.82,                           Last mean reward: 52.82\n",
      "Num timesteps: 3600\n",
      "Best mean reward: 52.82,                           Last mean reward: 52.82\n",
      "Num timesteps: 3620\n",
      "Best mean reward: 52.82,                           Last mean reward: 52.82\n",
      "Num timesteps: 3640\n",
      "Best mean reward: 52.82,                           Last mean reward: 53.47\n",
      "Saving new best model at 3636 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3660\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.47\n",
      "Num timesteps: 3680\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.13\n",
      "Num timesteps: 3700\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.13\n",
      "Num timesteps: 3720\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.13\n",
      "Num timesteps: 3740\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.13\n",
      "Num timesteps: 3760\n",
      "Best mean reward: 53.47,                           Last mean reward: 53.70\n",
      "Saving new best model at 3759 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3780\n",
      "Best mean reward: 53.70,                           Last mean reward: 53.70\n",
      "Num timesteps: 3800\n",
      "Best mean reward: 53.70,                           Last mean reward: 53.70\n",
      "Num timesteps: 3820\n",
      "Best mean reward: 53.70,                           Last mean reward: 53.70\n",
      "Num timesteps: 3840\n",
      "Best mean reward: 53.70,                           Last mean reward: 53.86\n",
      "Saving new best model at 3824 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3860\n",
      "Best mean reward: 53.86,                           Last mean reward: 53.86\n",
      "Num timesteps: 3880\n",
      "Best mean reward: 53.86,                           Last mean reward: 53.86\n",
      "Num timesteps: 3900\n",
      "Best mean reward: 53.86,                           Last mean reward: 53.86\n",
      "Num timesteps: 3920\n",
      "Best mean reward: 53.86,                           Last mean reward: 53.86\n",
      "Num timesteps: 3940\n",
      "Best mean reward: 53.86,                           Last mean reward: 53.86\n",
      "Num timesteps: 3960\n",
      "Best mean reward: 53.86,                           Last mean reward: 54.89\n",
      "Saving new best model at 3952 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 3980\n",
      "Best mean reward: 54.89,                           Last mean reward: 54.89\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 54.89,                           Last mean reward: 54.89\n",
      "Num timesteps: 4020\n",
      "Best mean reward: 54.89,                           Last mean reward: 54.96\n",
      "Saving new best model at 4012 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4040\n",
      "Best mean reward: 54.96,                           Last mean reward: 54.96\n",
      "Num timesteps: 4060\n",
      "Best mean reward: 54.96,                           Last mean reward: 54.96\n",
      "Num timesteps: 4080\n",
      "Best mean reward: 54.96,                           Last mean reward: 54.96\n",
      "Num timesteps: 4100\n",
      "Best mean reward: 54.96,                           Last mean reward: 55.32\n",
      "Saving new best model at 4094 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4120\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.32\n",
      "Num timesteps: 4140\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.32\n",
      "Num timesteps: 4160\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4180\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4200\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4220\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4240\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4260\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4280\n",
      "Best mean reward: 55.32,                           Last mean reward: 55.31\n",
      "Num timesteps: 4300\n",
      "Best mean reward: 55.32,                           Last mean reward: 56.33\n",
      "Saving new best model at 4281 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4320\n",
      "Best mean reward: 56.33,                           Last mean reward: 56.33\n",
      "Num timesteps: 4340\n",
      "Best mean reward: 56.33,                           Last mean reward: 56.33\n",
      "Num timesteps: 4360\n",
      "Best mean reward: 56.33,                           Last mean reward: 56.33\n",
      "Num timesteps: 4380\n",
      "Best mean reward: 56.33,                           Last mean reward: 56.33\n",
      "Num timesteps: 4400\n",
      "Best mean reward: 56.33,                           Last mean reward: 56.97\n",
      "Saving new best model at 4387 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4420\n",
      "Best mean reward: 56.97,                           Last mean reward: 56.97\n",
      "Num timesteps: 4440\n",
      "Best mean reward: 56.97,                           Last mean reward: 56.97\n",
      "Num timesteps: 4460\n",
      "Best mean reward: 56.97,                           Last mean reward: 56.97\n",
      "Num timesteps: 4480\n",
      "Best mean reward: 56.97,                           Last mean reward: 56.97\n",
      "Num timesteps: 4500\n",
      "Best mean reward: 56.97,                           Last mean reward: 57.47\n",
      "Saving new best model at 4483 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4520\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4540\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4560\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4580\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4600\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4620\n",
      "Best mean reward: 57.47,                           Last mean reward: 57.47\n",
      "Num timesteps: 4640\n",
      "Best mean reward: 57.47,                           Last mean reward: 58.73\n",
      "Saving new best model at 4640 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4660\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4680\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4700\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4720\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4740\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4760\n",
      "Best mean reward: 58.73,                           Last mean reward: 58.73\n",
      "Num timesteps: 4780\n",
      "Best mean reward: 58.73,                           Last mean reward: 59.60\n",
      "Saving new best model at 4768 timesteps to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4800\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4820\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4840\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4860\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4880\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4900\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4920\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4940\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4960\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 4980\n",
      "Best mean reward: 59.60,                           Last mean reward: 59.60\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 59.60,                           Last mean reward: 61.68\n",
      "Saving new best model at 4996 timesteps to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x17cf0f5d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1, monitor_dir=log_dir)\n",
    "# Equivalent to:\n",
    "# env = gym.make(\"Cartpole-v1\")\n",
    "# env = Monitor(env, log_dir)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create Callback\n",
    "callback = SaveBestModelCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
    "\n",
    "# Create model\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: realtime performance plotting\n",
    "\n",
    "Note: Stable-Baselines3 has Tensorboard support, but this can be cumbersome and disk-heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting performance in realtime.\n",
    "    \n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "\n",
    "        if self._plot is None:\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(6, 3))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, =ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02,\n",
    "                                     self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True, True, True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -52      |\n",
      "| time/              |          |\n",
      "|    fps             | 146      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 999        |\n",
      "|    ep_rew_mean          | -50        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00769101 |\n",
      "|    clip_fraction        | 0.0304     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.00248    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00116   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.92       |\n",
      "|    value_loss           | 0.0689     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -48.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004719085 |\n",
      "|    clip_fraction        | 0.0164      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.148      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00483     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00784    |\n",
      "|    std                  | 0.857       |\n",
      "|    value_loss           | 0.043       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17cfd5890>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"/tmp/gym\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "plotting_callback = PlottingCallback()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(5000, callback=plotting_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -51.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 6972     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -50         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4366        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006449253 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00179    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00524    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.922       |\n",
      "|    value_loss           | 0.0644      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -48.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3878        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006689742 |\n",
      "|    clip_fraction        | 0.0233      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00529    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 0.0335      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17edbcc90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1, monitor_dir=log_dir)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Code your own callback\n",
    "\n",
    "Save best model based on test environment evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "    \n",
    "    :param eval_env: (gym.Env)\n",
    "    :param n_eval_episodes: (int)\n",
    "    :param eval_freq: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.log_dir = \"/tmp/gym/\"\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        Called by model every step.\n",
    "        \n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n_calls automatically updated by BaseCallback\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # evaluate policy\n",
    "            rewards = []\n",
    "\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                total_reward = 0\n",
    "                done = False\n",
    "                obs = self.eval_env.reset()\n",
    "\n",
    "                while not done:\n",
    "                    action = self.model.predict(obs, deterministic=True)\n",
    "                    obs, reward, terminated, truncated, _info = self.eval_env.step(action)\n",
    "                    total_reward += reward\n",
    "                    done = terminated or truncated\n",
    "                \n",
    "                rewards.append(total_reward)\n",
    "\n",
    "            mean_reward = np.mean(rewards)\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.model.save(log_dir + \"best_model\")\n",
    "                print(f\"New best mean reward: {self.best_mean_reward:.2f}\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "array([0]) (<class 'numpy.ndarray'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# ====================== #\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# Train the RL model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39mint\u001b[39;49m(\u001b[39m100000\u001b[39;49m), callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    183\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 184\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     36\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(obs, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m     obs, reward, terminated, truncated, _info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     38\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     39\u001b[0m     done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:188\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:237\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    240\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/sb3/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 133\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(\n\u001b[1;32m    134\u001b[0m         action\n\u001b[1;32m    135\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: array([0]) (<class 'numpy.ndarray'>) invalid"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Env used for training\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "# Env for evaluating the agent\n",
    "eval_env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "\n",
    "# === YOUR CODE HERE ===#\n",
    "# Create the callback object\n",
    "callback = EvalCallback(eval_env=eval_env, n_eval_episodes=5, eval_freq=20)\n",
    "\n",
    "# Create the RL model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# ====================== #\n",
    "\n",
    "# Train the RL model\n",
    "model.learn(int(100000), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
